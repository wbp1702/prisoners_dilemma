{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "092f0990-8f3d-400e-91f9-a3df6eebb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e31a99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the probabilities of the actions  \n",
    "def boltzmann_exploration(qvalues, temperature):\n",
    "    # `- np.max(qvalues)` is used to prevent nan values \n",
    "    exp = np.exp((qvalues - np.max(qvalues)) / temperature)\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f65bfe0-7dd3-414c-9572-e8976b048bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd(i: bool, j: bool):\n",
    "    global num_outcomes\n",
    "    num_outcomes[int(i) + 2 * int(j)] += 1\n",
    "    \n",
    "    reward_table = [[(1, 1), (5, 0)], [(0, 5), (3, 3)]]\n",
    "    return reward_table[int(i)][int(j)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8bcc2",
   "metadata": {},
   "source": [
    "# Agent\n",
    "Each agent has two Q-Tables, one for partner selection (ps) and one for prisoners dilemma (pd).\n",
    "\n",
    "|  | Defect(0) | Cooperate(1) |\n",
    "| --- | --- | --- |\n",
    "| Partner Previously Defected(0) | (0, 0) | (0, 1) |\n",
    "| Partner Previously Cooperated(1) | (1, 0) | (1, 1) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15af4e93-ccc4-4543-8759-bf098502bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, learning_rate: float, temperature: float, discount_rate: float):\n",
    "        self.a = learning_rate\n",
    "        self.t = temperature\n",
    "        self.g = discount_rate\n",
    "        self.ps_s = False\n",
    "        self.ps_a = False\n",
    "        self.last_action = bool(random.getrandbits(1))\n",
    "        self.qvalues_ps = np.zeros((2, 2))\n",
    "        self.qvalues_pd = np.zeros((2, 2))\n",
    "        self.rewards_ps = np.zeros((2, 2))\n",
    "        self.rewards_pd = np.zeros((2, 2))\n",
    "        return\n",
    "\n",
    "    # get action for partner selection\n",
    "    # returns true if agent stays and false if agent breaks ties\n",
    "    def get_action_ps(self, partner_la) -> bool:\n",
    "        prob = boltzmann_exploration(self.qvalues_ps[int(partner_la)], self.t)\n",
    "        return bool(np.random.choice(2, p=prob))\n",
    "\n",
    "    # get action for the prisoner's dilemma game\n",
    "    # returns true if the agent cooperates and false if the agent defects\n",
    "    def get_action_pd(self, partner_la) -> bool:\n",
    "        prob = boltzmann_exploration(self.qvalues_pd[int(partner_la)], self.t)\n",
    "        return bool(np.random.choice(2, p=prob))\n",
    "\n",
    "    def update_reward(self, ps_s: bool, ps_a: bool, pd_s: bool, pd_a: bool, r):\n",
    "        self.rewards_ps[int(ps_s), int(ps_a)] += r\n",
    "        self.rewards_pd[int(pd_s), int(pd_a)] += r\n",
    "\n",
    "    # trains the agent using the Q-Learning formula\n",
    "    def train(self):\n",
    "        ps_qvalues = np.zeros((2, 2))\n",
    "        ps_qvalues[0, 0] = (1 - self.a) * self.qvalues_ps[0, 0] + self.a * (self.rewards_ps[0, 0] + self.g * np.max(self.qvalues_ps))\n",
    "        ps_qvalues[0, 1] = (1 - self.a) * self.qvalues_ps[0, 1] + self.a * (self.rewards_ps[0, 1] + self.g * np.max(self.qvalues_ps))\n",
    "        ps_qvalues[1, 0] = (1 - self.a) * self.qvalues_ps[1, 0] + self.a * (self.rewards_ps[1, 0] + self.g * np.max(self.qvalues_ps))\n",
    "        ps_qvalues[1, 1] = (1 - self.a) * self.qvalues_ps[1, 1] + self.a * (self.rewards_ps[1, 1] + self.g * np.max(self.qvalues_ps))\n",
    "        \n",
    "        pd_qvalues = np.zeros((2, 2))\n",
    "        pd_qvalues[0, 0] = (1 - self.a) * self.qvalues_pd[0, 0] + self.a * (self.rewards_pd[0, 0] + self.g * np.max(self.qvalues_pd))\n",
    "        pd_qvalues[0, 1] = (1 - self.a) * self.qvalues_pd[0, 1] + self.a * (self.rewards_pd[0, 1] + self.g * np.max(self.qvalues_pd))\n",
    "        pd_qvalues[1, 0] = (1 - self.a) * self.qvalues_pd[1, 0] + self.a * (self.rewards_pd[1, 0] + self.g * np.max(self.qvalues_pd))\n",
    "        pd_qvalues[1, 1] = (1 - self.a) * self.qvalues_pd[1, 1] + self.a * (self.rewards_pd[1, 1] + self.g * np.max(self.qvalues_pd))\n",
    "\n",
    "        self.qvalues_ps = ps_qvalues\n",
    "        self.qvalues_pd = pd_qvalues\n",
    "        self.rewards_ps = np.zeros((2, 2))\n",
    "        self.rewards_pd = np.zeros((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f028037-2a1d-4bc0-8472-05cb101cf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdoo(population: int, rounds: int, episodes: int, learning_rate: float, temperature: float, discount_rate: float):\n",
    "    agents = [Agent(learning_rate, temperature, discount_rate) for _ in range(population)]\n",
    "    unpaired = list(agents)\n",
    "    \n",
    "    pairs = []\n",
    "    while unpaired:\n",
    "        i = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        j = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        pairs.append((i, j))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        for round in range(rounds):\n",
    "            for pair in pairs:\n",
    "                i = pair[0]\n",
    "                j = pair[1]\n",
    "                la_i = i.last_action\n",
    "                la_j = j.last_action\n",
    "                i.ps_s = la_j\n",
    "                j.ps_s = la_i\n",
    "                a_i = i.get_action_ps(la_j)\n",
    "                a_j = j.get_action_ps(la_i)\n",
    "                i.ps_a = a_i\n",
    "                j.ps_a = a_j\n",
    "\n",
    "                if not a_i or not a_j:\n",
    "                    unpaired.append(i)\n",
    "                    unpaired.append(j)\n",
    "                    pairs.remove(pair)\n",
    "                \n",
    "            while unpaired:\n",
    "                i = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "                j = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "                pairs.append((i, j))\n",
    "\n",
    "            for pair in pairs:\n",
    "                i = pair[0]\n",
    "                j = pair[1]\n",
    "                la_i = i.last_action\n",
    "                la_j = j.last_action\n",
    "                a_i = i.get_action_pd(la_j)\n",
    "                a_j = j.get_action_pd(la_i)\n",
    "                r_i, r_j = pd(a_i, a_j)\n",
    "                i.last_action = a_i\n",
    "                j.last_action = a_j\n",
    "                i.update_reward(i.ps_s, i.ps_a, la_j, a_i, r_i)\n",
    "                j.update_reward(j.ps_s, j.ps_a, la_i, a_j, r_j)\n",
    "\n",
    "        for agent in agents:\n",
    "            agent.train()\n",
    "    \n",
    "    num_games = population * rounds * episodes / 2\n",
    "    print(\"games: %i\" % (num_games))\n",
    "    print(\"(D, D): %i\\t%f%%\" % (num_outcomes[0], 100 * num_outcomes[0] / num_games))\n",
    "    print(\"(C, D): %i\\t%f%%\" % (num_outcomes[1], 100 * num_outcomes[1] / num_games))\n",
    "    print(\"(D, C): %i\\t%f%%\" % (num_outcomes[2], 100 * num_outcomes[2] / num_games))\n",
    "    print(\"(C, C): %i\\t%f%%\" % (num_outcomes[3], 100 * num_outcomes[3] / num_games))\n",
    "    \n",
    "    strategies = [\n",
    "        # (\"Random-PS\", 0, np.array([[1, 1], [1, 1]])),\n",
    "        (\"Always-Stay\", 0, np.array([[0, 1], [0, 1]])),\n",
    "        (\"Out-for-Tat\", 0, np.array([[1, 0], [0, 1]])),\n",
    "        (\"Reverse-OFT\", 0, np.array([[0, 1], [1, 0]])),\n",
    "        (\"Always-Switch\", 0, np.array([[1, 0], [1, 0]])),\n",
    "        # (\"Random-PD\", 1, np.array([[1, 1], [1, 1]])),\n",
    "        (\"Always-Cooperate\", 1, np.array([[0, 1], [0, 1]])),\n",
    "        (\"Tit-for-Tat\", 1, np.array([[1, 0], [0, 1]])),\n",
    "        (\"Reverse-TFT\", 1, np.array([[0, 1], [1, 0]])),\n",
    "        (\"Always-Defect\", 1, np.array([[1, 0], [1, 0]])),\n",
    "    ]\n",
    "\n",
    "    # TODO: switch strategy and agent loops for better performance\n",
    "    for agent in agents:\n",
    "        print()\n",
    "        # print(agent.qvalues_ps)\n",
    "        # print(agent.qvalues_pd)\n",
    "        \n",
    "        temp = agent.qvalues_ps\n",
    "        # temp = np.array([boltzmann_exploration(temp[0], temperature), boltzmann_exploration(temp[1], temperature)])\n",
    "        print(temp)\n",
    "        temp = agent.qvalues_pd\n",
    "        # temp = np.array([boltzmann_exploration(temp[0], temperature), boltzmann_exploration(temp[1], temperature)])\n",
    "        print(temp)\n",
    "\n",
    "        angles = [[], []]\n",
    "        for strategy in strategies:\n",
    "            mat = agent.qvalues_pd if strategy[1] == 1 else agent.qvalues_ps\n",
    "            # mat = np.array([boltzmann_exploration(mat[0], temperature), boltzmann_exploration(mat[1], temperature)])\n",
    "            # print(mat)\n",
    "            mat2 = strategy[2]\n",
    "            norm = np.linalg.norm(mat)\n",
    "            strategy_norm = np.linalg.norm(mat2)\n",
    "            elm = np.multiply(mat, mat2)\n",
    "            prod = np.sum(elm)\n",
    "            angle = np.rad2deg(np.arccos(prod / (norm * strategy_norm)))\n",
    "            angles[strategy[1]].append(angle)\n",
    "        \n",
    "        min_ps = np.argmin(angles[0])\n",
    "        min_pd = np.argmin(angles[1])\n",
    "        print(\"PS-Strategy = %s (%9fdeg)\\t\\tPD-Strategy = %s (%9fdeg)\" % (strategies[min_ps][0], angles[0][min_ps], strategies[min_pd + len(angles[0])][0], angles[1][min_pd]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc3fd2eb-3bdb-46ab-9d55-19399cd7a64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games: 600000\n",
      "(D, D): 599354\t99.892333%\n",
      "(C, D): 239\t0.039833%\n",
      "(D, C): 263\t0.043833%\n",
      "(C, C): 144\t0.024000%\n",
      "\n",
      "[[2979.00704375 2999.00704375]\n",
      " [2979.00704375 2979.00704375]]\n",
      "[[3001.45 2981.45]\n",
      " [2981.45 2981.45]]\n",
      "PS-Strategy = Always-Stay (44.904157deg)\t\tPD-Strategy = Tit-for-Tat (44.904235deg)\n",
      "\n",
      "[[2977.79553844 2997.79553844]\n",
      " [2977.79553844 2977.79553844]]\n",
      "[[2999.94063048 2979.94063048]\n",
      " [2979.94063048 2979.94063048]]\n",
      "PS-Strategy = Always-Stay (44.904118deg)\t\tPD-Strategy = Tit-for-Tat (44.904187deg)\n",
      "\n",
      "[[2999.386275 2979.386275]\n",
      " [2979.386275 2979.386275]]\n",
      "[[3000.30251875 2980.30251875]\n",
      " [2980.30251875 2980.30251875]]\n",
      "PS-Strategy = Out-for-Tat (44.904169deg)\t\tPD-Strategy = Tit-for-Tat (44.904198deg)\n",
      "\n",
      "[[2980.40728125 3000.40728125]\n",
      " [2980.40728125 2980.40728125]]\n",
      "[[3000.24139375 2980.24139375]\n",
      " [2980.24139375 2980.24139375]]\n",
      "PS-Strategy = Always-Stay (44.904202deg)\t\tPD-Strategy = Tit-for-Tat (44.904196deg)\n",
      "\n",
      "[[2995.50215625 2975.50215625]\n",
      " [2975.50215625 2975.50215625]]\n",
      "[[2997.421875 2977.421875]\n",
      " [2977.421875 2977.421875]]\n",
      "PS-Strategy = Out-for-Tat (44.904044deg)\t\tPD-Strategy = Tit-for-Tat (44.904106deg)\n",
      "\n",
      "[[2981.4175 3001.4175]\n",
      " [2981.4175 2981.4175]]\n",
      "[[3000.65 2980.65]\n",
      " [2980.65 2980.65]]\n",
      "PS-Strategy = Always-Stay (44.904234deg)\t\tPD-Strategy = Tit-for-Tat (44.904209deg)\n",
      "\n",
      "[[2981.155 3001.155]\n",
      " [2981.155 2981.155]]\n",
      "[[3001.55 2981.55]\n",
      " [2981.55 2981.55]]\n",
      "PS-Strategy = Always-Stay (44.904226deg)\t\tPD-Strategy = Tit-for-Tat (44.904238deg)\n",
      "\n",
      "[[2982. 3002.]\n",
      " [2982. 2982.]]\n",
      "[[3000.52225 2980.52225]\n",
      " [2980.52225 2980.52225]]\n",
      "PS-Strategy = Always-Stay (44.904253deg)\t\tPD-Strategy = Tit-for-Tat (44.904205deg)\n",
      "\n",
      "[[2980.97675 3000.97675]\n",
      " [2980.97675 2980.97675]]\n",
      "[[3000.572125 2980.572125]\n",
      " [2980.572125 2980.572125]]\n",
      "PS-Strategy = Always-Stay (44.904220deg)\t\tPD-Strategy = Tit-for-Tat (44.904207deg)\n",
      "\n",
      "[[2995.22541302 2975.22541302]\n",
      " [2975.22541302 2975.22541302]]\n",
      "[[2997.347525 2977.347525]\n",
      " [2977.347525 2977.347525]]\n",
      "PS-Strategy = Out-for-Tat (44.904035deg)\t\tPD-Strategy = Tit-for-Tat (44.904103deg)\n",
      "\n",
      "[[2999.2150375 2979.2150375]\n",
      " [2979.2150375 2979.2150375]]\n",
      "[[2999.59695114 2979.59695114]\n",
      " [2979.59695114 2979.59695114]]\n",
      "PS-Strategy = Out-for-Tat (44.904163deg)\t\tPD-Strategy = Tit-for-Tat (44.904176deg)\n",
      "\n",
      "[[2979.5839875 2999.5839875]\n",
      " [2979.5839875 2979.5839875]]\n",
      "[[3000.63504406 2980.63504406]\n",
      " [2980.63504406 2980.63504406]]\n",
      "PS-Strategy = Always-Stay (44.904175deg)\t\tPD-Strategy = Tit-for-Tat (44.904209deg)\n",
      "\n",
      "[[2978.86237531 2998.86237531]\n",
      " [2978.86237531 2978.86237531]]\n",
      "[[2998.83621 2978.83621]\n",
      " [2978.83621 2978.83621]]\n",
      "PS-Strategy = Always-Stay (44.904152deg)\t\tPD-Strategy = Tit-for-Tat (44.904151deg)\n",
      "\n",
      "[[2999.35 2979.35]\n",
      " [2979.35 2979.35]]\n",
      "[[3001.45875 2981.45875]\n",
      " [2981.45875 2981.45875]]\n",
      "PS-Strategy = Out-for-Tat (44.904168deg)\t\tPD-Strategy = Tit-for-Tat (44.904235deg)\n",
      "\n",
      "[[3000.86025 2980.86025]\n",
      " [2980.86025 2980.86025]]\n",
      "[[3000.211125 2980.211125]\n",
      " [2980.211125 2980.211125]]\n",
      "PS-Strategy = Out-for-Tat (44.904216deg)\t\tPD-Strategy = Tit-for-Tat (44.904195deg)\n",
      "\n",
      "[[2999.6675 2979.6675]\n",
      " [2979.6675 2979.6675]]\n",
      "[[3001.1 2981.1]\n",
      " [2981.1 2981.1]]\n",
      "PS-Strategy = Out-for-Tat (44.904178deg)\t\tPD-Strategy = Tit-for-Tat (44.904224deg)\n",
      "\n",
      "[[2977.82925 2997.82925]\n",
      " [2977.82925 2977.82925]]\n",
      "[[2999.05670469 2979.05670469]\n",
      " [2979.05670469 2979.05670469]]\n",
      "PS-Strategy = Always-Stay (44.904119deg)\t\tPD-Strategy = Tit-for-Tat (44.904158deg)\n",
      "\n",
      "[[2997.69708125 2977.69708125]\n",
      " [2977.69708125 2977.69708125]]\n",
      "[[2999.67056875 2979.67056875]\n",
      " [2979.67056875 2979.67056875]]\n",
      "PS-Strategy = Out-for-Tat (44.904115deg)\t\tPD-Strategy = Tit-for-Tat (44.904178deg)\n",
      "\n",
      "[[2998.5 2978.5]\n",
      " [2978.5 2978.5]]\n",
      "[[2996.34506875 2976.34506875]\n",
      " [2976.34506875 2976.34506875]]\n",
      "PS-Strategy = Out-for-Tat (44.904140deg)\t\tPD-Strategy = Tit-for-Tat (44.904071deg)\n",
      "\n",
      "[[2999.8 2979.8]\n",
      " [2979.8 2979.8]]\n",
      "[[3000.5 2980.5]\n",
      " [2980.5 2980.5]]\n",
      "PS-Strategy = Out-for-Tat (44.904182deg)\t\tPD-Strategy = Tit-for-Tat (44.904205deg)\n"
     ]
    }
   ],
   "source": [
    "num_outcomes = [0, 0, 0, 0]\n",
    "\n",
    "sdoo(20, 20, 3000, 0.05, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecafa20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
