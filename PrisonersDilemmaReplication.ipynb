{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092f0990-8f3d-400e-91f9-a3df6eebb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31a99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the probabilities of the actions  \n",
    "def boltzmann_exploration(qvalues, temperature):\n",
    "    # `- np.max(qvalues)` is used to prevent nan values \n",
    "    exp = np.exp((qvalues - np.max(qvalues)) / temperature)\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f65bfe0-7dd3-414c-9572-e8976b048bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd(i: bool, j: bool):\n",
    "    global num_outcomes\n",
    "    num_outcomes[int(i) + 2 * int(j)] += 1\n",
    "    \n",
    "    reward_table = [[(1, 1), (5, 0)], [(0, 5), (3, 3)]]\n",
    "    return reward_table[int(i)][int(j)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8bcc2",
   "metadata": {},
   "source": [
    "# Agent\n",
    "Each agent has two Q-Tables, one for partner selection (ps) and one for prisoners dilemma (pd).\n",
    "\n",
    "|  | Defect(0) | Cooperate(1) |\n",
    "| --- | --- | --- |\n",
    "| Partner Previously Defected(0) | (0, 0) | (0, 1) |\n",
    "| Partner Previously Cooperated(1) | (1, 0) | (1, 1) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15af4e93-ccc4-4543-8759-bf098502bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, learning_rate: float, temperature: float, discount_rate: float):\n",
    "        self.a = learning_rate\n",
    "        self.t = temperature\n",
    "        self.g = discount_rate\n",
    "        self.last_action = bool(random.getrandbits(1))\n",
    "        self.qvalues_ps = np.zeros((2, 2))\n",
    "        self.qvalues_pd = np.zeros((2, 2))\n",
    "        self.rewards = np.zeros((2, 2))\n",
    "        return\n",
    "\n",
    "    # get action for partner selection\n",
    "    # returns true if agent stays and false if agent breaks ties\n",
    "    def get_action_ps(self, partner_la) -> bool:\n",
    "        prob = boltzmann_exploration(self.qvalues_ps[int(partner_la)], self.t)\n",
    "        return bool(np.random.choice(2, p=prob))\n",
    "\n",
    "    # get action for the prisoner's dilemma game\n",
    "    # returns true if the agent cooperates and false if the agent defects\n",
    "    def get_action_pd(self, partner_la) -> bool:\n",
    "        prob = boltzmann_exploration(self.qvalues_pd[int(partner_la)], self.t)\n",
    "        return bool(np.random.choice(2, p=prob))\n",
    "\n",
    "    def update_reward(self, r, pd_a: bool, pla: bool):\n",
    "        self.rewards[int(pla), int(pd_a)] = r\n",
    "\n",
    "    # trains the agent using the Q-Learning formula\n",
    "    def train(self):\n",
    "        ps_qvalues = np.zeros((2, 2))\n",
    "        # NOTE: no reward is given in the partner selection stage as indicated in the paper (page 1113, 5th line of 2nd paragraph)\n",
    "        ps_qvalues[0, 0] = (1 - self.a) * self.qvalues_ps[0, 0] + self.a * (0 + self.g * np.max(self.qvalues_ps))\n",
    "        ps_qvalues[0, 1] = (1 - self.a) * self.qvalues_ps[0, 1] + self.a * (0 + self.g * np.max(self.qvalues_ps))\n",
    "        ps_qvalues[1, 0] = (1 - self.a) * self.qvalues_ps[1, 0] + self.a * (0 + self.g * np.max(self.qvalues_ps))\n",
    "        ps_qvalues[1, 1] = (1 - self.a) * self.qvalues_ps[1, 1] + self.a * (0 + self.g * np.max(self.qvalues_ps))\n",
    "        \n",
    "        pd_qvalues = np.zeros((2, 2))\n",
    "        pd_qvalues[0, 0] = (1 - self.a) * self.qvalues_pd[0, 0] + self.a * (self.rewards[0, 0] + self.g * np.max(self.qvalues_pd))\n",
    "        pd_qvalues[0, 1] = (1 - self.a) * self.qvalues_pd[0, 1] + self.a * (self.rewards[0, 1] + self.g * np.max(self.qvalues_pd))\n",
    "        pd_qvalues[1, 0] = (1 - self.a) * self.qvalues_pd[1, 0] + self.a * (self.rewards[1, 0] + self.g * np.max(self.qvalues_pd))\n",
    "        pd_qvalues[1, 1] = (1 - self.a) * self.qvalues_pd[1, 1] + self.a * (self.rewards[1, 1] + self.g * np.max(self.qvalues_pd))\n",
    "\n",
    "        self.qvalues_ps = ps_qvalues\n",
    "        self.qvalues_pd = pd_qvalues\n",
    "        self.rewards = np.zeros((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f028037-2a1d-4bc0-8472-05cb101cf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdoo(population: int, rounds: int, episodes: int, learning_rate: float, temperature: float, discount_rate: float):\n",
    "    agents = [Agent(learning_rate, temperature, discount_rate) for _ in range(population)]\n",
    "    unpaired = list(agents)\n",
    "    \n",
    "    pairs = []\n",
    "    while unpaired:\n",
    "        i = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        j = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "        pairs.append((i, j))\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        for round in range(rounds):\n",
    "            for pair in pairs:\n",
    "                i = pair[0]\n",
    "                j = pair[1]\n",
    "                la_i = i.last_action\n",
    "                la_j = j.last_action\n",
    "                a_i = i.get_action_ps(la_j)\n",
    "                a_j = j.get_action_ps(la_i)\n",
    "\n",
    "                if not a_i or not a_j:\n",
    "                    unpaired.append(i)\n",
    "                    unpaired.append(j)\n",
    "                    pairs.remove(pair)\n",
    "                \n",
    "            while unpaired:\n",
    "                i = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "                j = unpaired.pop(np.random.randint(len(unpaired)))\n",
    "                pairs.append((i, j))\n",
    "\n",
    "            for pair in pairs:\n",
    "                i = pair[0]\n",
    "                j = pair[1]\n",
    "                la_i = i.last_action\n",
    "                la_j = j.last_action\n",
    "                a_i = i.get_action_pd(la_j)\n",
    "                a_j = j.get_action_pd(la_i)\n",
    "                r_i, r_j = pd(a_i, a_j)\n",
    "                i.last_action = a_i\n",
    "                j.last_action = a_j\n",
    "                i.update_reward(r_i, a_i, la_j)\n",
    "                j.update_reward(r_j, a_j, la_i)\n",
    "        for agent in agents:\n",
    "            agent.train()\n",
    "    \n",
    "    num_games = population * rounds * episodes / 2\n",
    "    print(\"games: %i\" % (num_games))\n",
    "    print(\"(D, D): %i\\t%f%%\" % (num_outcomes[0], 100 * num_outcomes[0] / num_games))\n",
    "    print(\"(C, D): %i\\t%f%%\" % (num_outcomes[1], 100 * num_outcomes[1] / num_games))\n",
    "    print(\"(D, C): %i\\t%f%%\" % (num_outcomes[2], 100 * num_outcomes[2] / num_games))\n",
    "    print(\"(C, C): %i\\t%f%%\" % (num_outcomes[3], 100 * num_outcomes[3] / num_games))\n",
    "    \n",
    "    strategies = [\n",
    "        # (\"Random-PS\", 0, np.array([[1, 1], [1, 1]])),\n",
    "        (\"Always-Stay\", 0, np.array([[0, 1], [0, 1]])),\n",
    "        (\"Out-for-Tat\", 0, np.array([[1, 0], [0, 1]])),\n",
    "        (\"Reverse-OFT\", 0, np.array([[0, 1], [1, 0]])),\n",
    "        (\"Always-Switch\", 0, np.array([[1, 0], [1, 0]])),\n",
    "        # (\"Random-PD\", 1, np.array([[1, 1], [1, 1]])),\n",
    "        (\"Always-Cooperate\", 1, np.array([[0, 1], [0, 1]])),\n",
    "        (\"Tit-for-Tat\", 1, np.array([[1, 0], [0, 1]])),\n",
    "        (\"Reverse-TFT\", 1, np.array([[0, 1], [1, 0]])),\n",
    "        (\"Always-Defect\", 1, np.array([[1, 0], [1, 0]])),\n",
    "    ]\n",
    "\n",
    "    # TODO: switch strategy and agent loops for better performance\n",
    "    for agent in agents:\n",
    "        print()\n",
    "        # print(agent.qvalues_ps)\n",
    "        # print(agent.qvalues_pd)\n",
    "        \n",
    "        temp = agent.qvalues_ps\n",
    "        temp = np.array([boltzmann_exploration(temp[0], temperature), boltzmann_exploration(temp[1], temperature)])\n",
    "        print(temp)\n",
    "        temp = agent.qvalues_pd\n",
    "        temp = np.array([boltzmann_exploration(temp[0], temperature), boltzmann_exploration(temp[1], temperature)])\n",
    "        print(temp)\n",
    "\n",
    "        angles = [[], []]\n",
    "        for strategy in strategies:\n",
    "            mat = agent.qvalues_pd if strategy[1] == 1 else agent.qvalues_ps\n",
    "            mat = np.array([boltzmann_exploration(mat[0], temperature), boltzmann_exploration(mat[1], temperature)])\n",
    "            # print(mat)\n",
    "            mat2 = strategy[2]\n",
    "            norm = np.linalg.norm(mat)\n",
    "            strategy_norm = np.linalg.norm(mat2)\n",
    "            elm = np.multiply(mat, mat2)\n",
    "            prod = np.sum(elm)\n",
    "            angle = np.rad2deg(np.arccos(prod / (norm * strategy_norm)))\n",
    "            angles[strategy[1]].append(angle)\n",
    "        \n",
    "        min_ps = np.argmin(angles[0])\n",
    "        min_pd = np.argmin(angles[1])\n",
    "        print(\"PS-Strategy = %s (%9fdeg)\\t\\tPD-Strategy = %s (%9fdeg)\" % (strategies[min_ps][0], angles[0][min_ps], strategies[min_pd + len(angles[0])][0], angles[1][min_pd]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3fd2eb-3bdb-46ab-9d55-19399cd7a64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games: 600000\n",
      "(D, D): 378905\t63.150833%\n",
      "(C, D): 97618\t16.269667%\n",
      "(D, C): 97713\t16.285500%\n",
      "(C, C): 25764\t4.294000%\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.81306622 0.18693378]\n",
      " [0.57744365 0.42255635]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (26.544527deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.82301278 0.17698722]\n",
      " [0.73742537 0.26257463]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (16.287082deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.84818014 0.15181986]\n",
      " [0.70723591 0.29276409]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (17.392766deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.86916047 0.13083953]\n",
      " [0.78527841 0.21472159]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (12.452972deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.77916107 0.22083893]\n",
      " [0.81720612 0.18279388]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (14.309912deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.75694725 0.24305275]\n",
      " [0.7220771  0.2779229 ]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (19.484660deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.78206117 0.21793883]\n",
      " [0.75301879 0.24698121]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (16.911541deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.74291448 0.25708552]\n",
      " [0.85107843 0.14892157]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (15.226401deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.78647112 0.21352888]\n",
      " [0.80283775 0.19716225]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (14.510800deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.65969414 0.34030586]\n",
      " [0.74987416 0.25012584]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (23.196795deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.88361126 0.11638874]\n",
      " [0.77180143 0.22819857]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (12.899829deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.81189977 0.18810023]\n",
      " [0.74632701 0.25367299]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (16.155726deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.86887842 0.13112158]\n",
      " [0.80265921 0.19734079]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (11.548489deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.85844774 0.14155226]\n",
      " [0.72607032 0.27392968]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (16.045367deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.77479089 0.22520911]\n",
      " [0.79875794 0.20124206]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (15.209232deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.767451   0.232549  ]\n",
      " [0.79832542 0.20167458]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (15.574221deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.69213085 0.30786915]\n",
      " [0.69482256 0.30517744]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (23.846293deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.80494185 0.19505815]\n",
      " [0.79407316 0.20592684]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (14.087685deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.86583249 0.13416751]\n",
      " [0.70699681 0.29300319]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (17.058898deg)\n",
      "\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "[[0.87108657 0.12891343]\n",
      " [0.77060409 0.22939591]]\n",
      "PS-Strategy = Always-Stay (45.000000deg)\t\tPD-Strategy = Always-Defect (13.213379deg)\n"
     ]
    }
   ],
   "source": [
    "num_outcomes = [0, 0, 0, 0]\n",
    "\n",
    "sdoo(20, 20, 3000, 0.05, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecafa20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
